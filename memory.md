```c

1. What is memory management in system programming?

	Memory management is the process by which an operating system handles allocation, tracking, and deallocation of memory space to processes, ensuring efficient and safe use of RAM.

2. Define virtual memory.
	
 	Virtual memory is an abstraction that allows processes to use more memory than physically available by extending RAM with disk space, and by isolating       process memory spaces.

3. Differentiate between physical memory and virtual memory.
	
    Physical Memory	Virtual Memory
	Actual RAM	Logical view of memory
	Limited in size	Can be larger (uses disk)
	Shared among processes	Isolated per process
	Directly accessed by CPU	Mapped via MMU

4. What is the role of an operating system in memory management?
	The OS:
	
	Allocates memory to processes
	
	Tracks used/free memory
	
	Swaps memory to disk (paging)
	
	Manages isolation between processes
	
	Handles fragmentation

5. Explain the purpose of memory allocation.

	Memory allocation reserves a portion of memory (RAM or virtual) for use by processes to store code, data, and runtime variables.

6. Describe the significance of memory deallocation.

	Memory deallocation frees up memory when it's no longer needed by a process, preventing memory leaks and allowing reuse.

7. Define fragmentation in memory management.

	Fragmentation is the condition where free memory is divided into small blocks and is unusable for larger allocations, reducing efficiency.

8. What are the types of fragmentation?
	
    Internal Fragmentation
	
	External Fragmentation

9. Explain internal fragmentation.
	
    Occurs when allocated memory exceeds the requested amount, leaving unused space inside allocated blocks.
	
	Example: Allocating 100 bytes when only 85 are needed.

10. Explain external fragmentation.

	Occurs when free memory is broken into non-contiguous blocks, making it hard to allocate large memory even if enough total memory exists.

11. How is fragmentation managed in memory allocation?
	
    Paging eliminates external fragmentation.
	
	Segmentation with compaction can reduce fragmentation.
	
	Memory pools/slabs reduce internal fragmentation.

    Allocators like malloc() often use best-fit, first-fit, buddy systems to manage fragmentation.

12. Describe the concept of paging.

	Paging divides virtual memory into fixed-size pages and physical memory into frames. Pages are mapped to frames using a page table, enabling non-contiguous memory allocation.

13. Explain segmentation.

	Segmentation divides memory based on logical segments like code, data, and stack. Each segment has a base and limit, offering logical memory protection.

14. What is the difference between paging and segmentation?

	Paging	Segmentation
	Fixed-size blocks (pages)	Variable-size segments
	Solves external fragmentation	May still suffer from it
	Uniform structure	Based on logical program parts
	Page table used	Segment table used

15. Define page table.

	A page table maps virtual page numbers to physical frame numbers and stores control bits (valid, dirty, access rights).

16. Define Memory Management Unit (MMU).

	MMU is a hardware device that translates virtual addresses to physical addresses using page tables and handles memory protection.

17. Explain the role of MMU in memory management.

	The MMU:
	
	Performs address translation (VA → PA)
	
	Enforces access control and isolation
	
	Uses TLB for faster access
	
	Handles page faults (with help of OS)

18. Describe the translation lookaside buffer (TLB).

	TLB is a cache inside the MMU that stores recent page table entries to speed up virtual-to-physical address translation.

19. What is TLB miss? How is it handled?

	A TLB miss occurs when the requested page is not found in the TLB. It is handled by:
	
	Accessing the page table in RAM
	
	Updating the TLB with the entry
	
	Continuing with address translation

20. Discuss the working principle of MMU.

	The MMU:
	
	Receives virtual address from CPU
	
	Checks TLB for cached mapping
	
	If miss, uses page table to get frame number
	
	Translates VA → PA
	
	Enforces access permissions
	
	Triggers page fault if invalid

21. Explain the concept of address translation in MMU.

	Address translation is the process of converting a virtual address (VA) generated by the CPU into a physical address (PA) using the Memory Management Unit (MMU).
	
	Steps:
	
	CPU generates a virtual address.
	
	MMU checks the Translation Lookaside Buffer (TLB).
	
	If found (TLB hit), it gets the corresponding physical frame.
	
	If not (TLB miss), MMU walks the page table to find the mapping.
	
	The physical address is formed by combining the frame number with the page offset.

22. How does MMU support virtual memory?

	MMU enables virtual memory by:
	
	Mapping virtual addresses to physical addresses dynamically via page tables.
	
	Allowing non-contiguous physical memory for a process.
	
	Providing isolation between processes.
	
	Handling page faults to bring missing pages from disk (swap).
	
	Using TLB for faster access.

23. Describe the process of page table traversal in MMU.

	When there is a TLB miss, MMU must traverse the page table:
	
	Single-level paging: Use the page number part of VA to index into the page table and fetch the frame number.
	
	Multi-level paging (used in modern OSes):
	
	VA is split into multiple parts (e.g., page directory index, page table index, offset).
	
	MMU first reads the page directory, then the page table, and finally gets the frame number.
	
	Final physical address = frame number + page offset.

24. What is page fault handling in MMU?

	A page fault occurs when the page requested by the CPU is not present in physical memory.
	
	Page fault handling steps:
	
	MMU raises a page fault exception.
	
	OS traps the fault and checks if the access is valid.
	
	If valid, OS loads the page from disk/swap into RAM.
	
	Page table is updated.
	
	TLB is updated (optional).
	
	Instruction is re-executed.

25. Explain the page replacement algorithms used in MMU.

	Page replacement algorithms decide which memory page to evict when a new page needs to be loaded but memory is full.
	
	Common algorithms:
	
	FIFO (First-In, First-Out)
	
	Optimal
	
	LRU (Least Recently Used)
	
	Clock (Second-Chance)
	
	NFU/Aging (Not Frequently Used)

26. Define page replacement algorithms.

	Page replacement algorithms are strategies used by the OS to select a victim page to remove from RAM when handling a page fault and RAM is full.
	
	The goal is to minimize page faults.

27. Describe the FIFO page replacement algorithm.

	Evicts the oldest page in memory (the one loaded first).
	
	Simple and fast but not always efficient.
	
	Example:
	Pages loaded: A → B → C → D (Memory full)
	New page E → Evict A (first-in)

28. Discuss the optimal page replacement algorithm.
	
	Replaces the page that will not be used for the longest time in the future.
	
	Gives lowest possible page faults.
	
	Not implementable in practice (requires future knowledge).
	
	Used as a benchmark to compare other algorithms.

29. Explain the LRU (Least Recently Used) page replacement algorithm.
	
 	Evicts the page that was least recently accessed.
	
	Assumes that pages used recently will be used again soon (temporal locality).
	
	Can be implemented using:
	
	Counters/timestamps
	
	Stack or linked list

Approximation using the Clock algorithm

30. What is the Clock page replacement algorithm?

	The Clock algorithm is a circular approximation of LRU:
	
	Pages are arranged in a circular list with a reference bit.
	
	A clock hand points to the oldest page.
	
	When a page fault occurs:
	
	If reference bit = 0 → replace page.
	
	If reference bit = 1 → clear it and move to next.
	
	Continues until a page with reference bit = 0 is found.

31. Advantages and Disadvantages of Page Replacement Algorithms

	Algorithm							Advantages									   Disadvantages
	FIFO							Simple to implement							Can evict heavily used pages
	Optimal						 Best possible performance					    Not implementable in practice
	LRU                  	    Good approximation to optimal	                Costly to implement (tracking access)
	Clock	                    Efficient approximation of LRU	           Still not perfect, can give sub-optimal performance
	Second Chance	           Improves FIFO by checking usage	                May degrade to FIFO if all bits are 1
	NRU	                         Supports multiple criteria	               May not choose best page for current workload

32. Compare and Contrast Different Page Replacement Algorithms

	Feature	             FIFO	        LRU	          Optimal	           Clock     	Second Chance
	Simplicity	         High	       Medium	        Low	              Medium	       Medium
	Performance	         Poor 	        Good	        Best 	           Good	        Better than FIFO
	Tracking Needed	     None	   Access history	Future knowledge   Reference bits	Reference bits
	Practicality	     Yes	   With overhead	     No	                Yes	            Yes

33. Explain the NRU (Not Recently Used) Page Replacement Algorithm
	Pages are classified into four classes based on:
	
	R bit (Referenced)
	
	M bit (Modified)
	
	Classes:
	
	Class 0: R=0, M=0
	
	Class 1: R=0, M=1
	
	Class 2: R=1, M=0
	
	Class 3: R=1, M=1
	
	NRU chooses a page from lowest class for replacement.

    OS periodically clears R bits to update usage.

34. Describe the Second Chance Page Replacement Algorithm

	Enhancement of FIFO using a reference bit:
	
	When replacing:
	
	If reference bit = 1 → clear it, give second chance, move to next.
	
	If reference bit = 0 → replace it.
	
	Works like Clock, but can be implemented linearly.

35. Enhancements to Basic Page Replacement Algorithms

	Aging Algorithm: Uses counters to track recent use (approximate LRU).
	
	Working Set Model: Keeps track of pages used in recent window of time.
	
	Adaptive Algorithms: Change behavior based on workload (e.g., Linux uses LRU variants like LRU-K, CLOCK-Pro).
	
	Combining page replacement with prefetching: Anticipate needed pages.

36. Define Segmentation in Memory Management

	Segmentation divides memory into logical units (segments) like code, data, stack.
	Each segment has:
	
	A base address (start of segment)
	
	A limit (length of segment)

37. Benefits of Segmentation

	Reflects logical structure of programs.
	
	Supports modular code, with separate segments.
	
	Enables sharing (e.g., shared libraries).
	
	Provides fine-grained protection (per-segment).

38. Disadvantages of Segmentation

	Suffers from external fragmentation.
	
	Complex to manage compared to paging.
	
	Slower than paging in address translation.

39. Implementation of Segmentation

	Each process has a segment table.
	
	Each entry contains:
	
	Base: physical starting address
	
	Limit: size of segment
	
	Logical address = (segment number, offset)
	
	MMU adds offset to base to get physical address, if offset < limit.

40. Segmentation Fault and Its Causes

	A segmentation fault (segfault) occurs when a program:
	
	Accesses memory outside the segment's limit.
	
	Tries to write to a read-only segment.
	
	Dereferences an invalid pointer (e.g., NULL, dangling).

41. Concept of Segment Registers

	Used in x86 architecture.
	
	Hold the base addresses of segments.
	
	Examples: CS (code segment), DS (data), SS (stack).
	
	Combined with offsets to form linear address.

42. What is a Segment Table?

	A segment table maps:
	
	Segment numbers → Base and Limit
	Used by MMU to:
	
	Perform bounds checking
	
	Compute physical address from segment + offset.

43. How Segmentation Supports Protection and Sharing

	Protection: Each segment has access permissions (e.g., read-only, execute).
	
	Sharing: Common segments (like libraries) can be mapped into multiple processes.

44. Discuss Segmentation with Paging

	Hybrid approach:
	
	Memory divided into segments, each segment further divided into pages.
	
	Used in x86 architecture (Protected Mode).
	
	Benefits:
	
	Logical structuring via segmentation
	
	No external fragmentation via paging

45. Compare and Contrast Segmentation with Paging

	Feature							Segmentation						Paging
	Division				Logical units (code/data/stack)		   Fixed-size blocks
	Fragmentation	           External fragmentation	        Internal fragmentation
	Address parts	           Segment number + offset	         Page number + offset
	Use case	              Modular programs, protection	   Efficient memory allocation
	Sharing	                   Easier for code segments	    Harder (needs identical page mappings)

46. Define memory fragmentation.

	Memory fragmentation occurs when free memory is broken into small, non-contiguous blocks, making it difficult to allocate large blocks of memory even if total free memory is sufficient.

47. Explain the causes of memory fragmentation.
	
 	Frequent allocation/deallocation of memory blocks of different sizes.
	
	Variable-sized memory requests.
	
	Long-running processes that allocate and free memory irregularly.
	
	Lack of efficient memory reuse mechanisms.

48. How does memory fragmentation affect system performance?

	Increases memory allocation failure rates.
	
	Leads to inefficient memory usage.
	
	Causes slower allocation/deallocation times.
	
	May force the system to use swap/disk, degrading performance.

49. Discuss the techniques to reduce memory fragmentation.

	Paging: Uses fixed-size pages to eliminate external fragmentation.
	
	Segmentation with paging: Hybrid method for flexibility and efficiency.
	
	Memory compaction: Reorganizes memory to consolidate free blocks.
	
	Memory pools (slab/slub allocators): Predefine fixed-size blocks.
	
	Best-fit, first-fit, next-fit allocation strategies.

50. Explain compaction as a technique for reducing fragmentation.

	Compaction moves all allocated memory blocks together and combines the free spaces into one large block.
	
	Reduces external fragmentation, especially in variable-size memory allocation schemes.

51. What is memory compaction?

	Memory compaction is a process in which:
	
	The OS shifts memory contents to group allocated blocks together.
	
	The holes (free spaces) are merged into a single large block.
	
	Requires updating pointers and process page tables.

52. Describe the working of memory compaction algorithms.

	Identify free holes and allocated blocks.
	
	Move allocated blocks to lower memory addresses.
	
	Update all references/pointers.
	
	Merge the remaining free space.

	Common in systems without paging (like embedded RTOS) or where fragmentation is severe.

53. Discuss the challenges in implementing memory compaction.

	Time-consuming and CPU-intensive.
	
	Risk of data corruption if pointers are not correctly updated.
	
	Not suitable for systems with real-time constraints.
	
	Requires suspending processes, affecting responsiveness.

54. Explain memory fragmentation in the context of embedded systems.

	Embedded systems often have:

		Limited RAM
		
		No virtual memory
		
		Fragmentation can quickly exhaust usable memory.
		
		Often use:
		
		Static memory allocation
		
		Memory pools
		
		Fixed-size buffers
		To avoid fragmentation altogether.

55. How does memory allocation impact memory fragmentation?
	
 	Dynamic allocation (e.g., malloc()/free()) increases the risk of fragmentation.
	
	Allocation strategies:
	
	First-fit: May leave small holes (external fragmentation).
	
	Best-fit: Reduces unused space, but can still fragment.
	
	Fixed-size block allocation reduces fragmentation.

56. Define memory mapping.

	Memory mapping is the process of associating a file or device into a process’s address space, allowing file I/O through memory access.

57. Explain the purpose of memory mapping.

	Speeds up file I/O by avoiding explicit read()/write().
	
	Enables shared memory between processes.
	
	Allows device memory access (e.g., GPU, memory-mapped I/O).
	
	Efficient loading of large files.

58. Describe the memory mapping techniques.

	File-backed mapping: Maps a file into memory using mmap().
	
	Anonymous mapping: No file backing (e.g., shared memory).
	
	Shared mapping: Changes reflect across processes.
	
	Private mapping: Copy-on-write behavior.

59. What is memory-mapped I/O?

	A technique where hardware device registers or memory are mapped to specific memory addresses.
	
	CPU can access device registers as if they were regular memory.
	
	Used in embedded systems, microcontrollers, and device drivers.
	
	Example: Accessing GPIO or UART via memory-mapped addresses.

60. Explain memory-mapped files.
	
	Memory-mapped files allow a file or portion of it to be mapped into a process's virtual address space. This enables file data to be accessed like regular memory, using pointers, without read() or write() calls.
	
	Implemented via mmap() in UNIX/Linux.
	
	Used in shared memory and large file I/O operations.

61. Discuss the advantages of memory mapping.

	Faster I/O: Avoids system call overhead (read/write).
	
	Shared memory: Allows multiple processes to share data.
	
	Simplified access: Use pointers instead of explicit I/O.
	
	On-demand loading: OS loads pages lazily when accessed.
	
	Memory-efficient: Automatically managed via page faults.

62. What are the drawbacks of memory mapping?
	
 	Limited address space in 32-bit systems.
	
	Complex error handling (segmentation faults on bad access).
	
	May not be efficient for small or random writes.
	
	Compatibility issues with non-POSIX systems.
	
	Requires OS support and file system permission.

63. How does memory mapping improve performance?

	Reduces copying between user space and kernel.
	
	Enables lazy loading (pages brought in on demand).
	
	Faster context switching in file I/O.
	
	Efficient for large and sequential reads/writes.

64. Explain memory-mapped graphics.

	Used in graphics hardware where the framebuffer is mapped to memory.
	
	Allows applications to draw directly to video memory using memory addresses.
	
	Common in embedded systems (e.g., display controllers).
	
	Speeds up rendering by avoiding intermediate buffers.

65. Discuss memory mapping in embedded systems.

	Used to:

		Access peripheral registers (UART, GPIO) directly.
		
		Handle framebuffers in display applications.
		
		Avoids system call overhead, useful in real-time systems.
		
		Often involves fixed memory addresses defined in hardware manuals.

66. Define cache memory.

	Cache memory is a small, high-speed memory located between the CPU and main memory that stores frequently accessed data and instructions to reduce access time.

67. Explain the purpose of cache memory.

	Bridges the speed gap between CPU and RAM.
	
	Stores recently or frequently accessed data.
	
	Reduces memory latency and improves performance.
	
	Supports instruction and data fetch efficiency.

68. Describe the types of cache memory.

	L1 Cache:
	
		Closest to CPU, smallest and fastest.
		
		Separate for data (L1d) and instructions (L1i).
	
	L2 Cache:
	
		Larger, slightly slower, often shared per core.
	
	L3 Cache:
	
		Shared among all cores.
		
		Improves multi-core performance.
		
		Instruction vs Data Cache:
		
		Some CPUs have separate caches for each.

69. Discuss the cache coherence problem.
	Occurs in multi-core systems when each core has its own cache.
	
	When one core updates its cached data, others may have stale copies.
	
	Requires cache coherence protocols like:
	
	MESI (Modified, Exclusive, Shared, Invalid)
	
	MOESI, MSI

70. Explain cache replacement policies.
	
 	Policies used to decide which cache line to evict when new data is loaded:
	
	LRU (Least Recently Used) – Common and effective.
	
	FIFO (First-In, First-Out) – Simple but not optimal.
	
	Random – Quick but unpredictable.
	
	LFU (Least Frequently Used) – Tracks access frequency.

71. What is cache associativity?

	Defines how memory blocks map to cache lines:
	
	Direct-mapped: One memory block maps to one cache line.
	
	Set-associative: A block maps to a set of cache lines (e.g., 4-way).
	
	Fully-associative: Any block can go into any cache line.
	
	Higher associativity = fewer conflicts, but more hardware complexity.

72. Describe the working of cache memory.

	CPU requests data.
	
	Cache checks if data is available (cache hit).
	
	If yes → data returned immediately.
	
	If no (cache miss) → fetch from RAM → place in cache → return to CPU.
	
	Uses replacement policy if cache is full.

73. Explain cache hit and cache miss.

	Cache Hit: Requested data is found in cache → fast access.
	
	Cache Miss: Data not in cache → must fetch from RAM → slower.
	
	Types of misses:
	
	Compulsory (first-time access)
	
	Capacity (cache too small)
	
	Conflict (same index for multiple blocks)

74. Discuss the importance of cache memory in memory management.

	Reduces average memory access time.
	
	Improves CPU throughput.
	
	Minimizes the impact of RAM latency.
	
	Plays a role in virtual memory and page caching (e.g., disk cache).

75. How does cache memory relate to memory hierarchy?

	Cache is the top level in the memory hierarchy:
	
	Registers > L1 Cache > L2 Cache > L3 Cache > RAM > Disk (swap)
	It provides fast, temporary storage for data closer to the CPU.
	
	Helps maintain the illusion of fast and large memory.

76. Define memory protection.

	Memory protection is a mechanism that restricts unauthorized access to memory areas, ensuring that processes cannot access or modify each other's memory or critical OS regions.

77. Explain the need for memory protection.

	Prevents processes from corrupting each other's data.
	
	Protects the operating system and kernel memory.
	
	Enforces process isolation.
	
	Helps catch programming bugs (e.g., segmentation faults).
	
	Improves system reliability and security.

78. Techniques for implementing memory protection:

	Virtual memory with per-process address spaces.
	
	Page tables with access permissions (R/W/X).
	
	Segmentation with base and limit checking.
	
	Hardware privilege levels (user/kernel mode).
	
	Memory access flags (NX, read-only, etc.).

79. What is segmentation fault?

	A segmentation fault (segfault) is an error caused by a program trying to:
	
	Access invalid memory (e.g., dereference a NULL or freed pointer).
	
	Write to read-only memory.
	
	Access memory outside allocated bounds.

80. Role of privilege levels in memory protection:

	CPU privilege levels (rings):
	
	Ring 0: Kernel mode — full access.
	
	Ring 3: User mode — restricted access.
	
	Prevents user-mode code from executing privileged instructions or accessing kernel memory.

81. Mechanism of memory protection in modern OS:

	Page-based protection:Each page has permission bits (read/write/execute).
	
	Per-process virtual memory:Isolates memory spaces.
	
	Hardware enforcement:MMU and TLB use protection bits.
	
	System calls:Only way to request kernel operations (controlled access).

82. Security implications of memory protection:
	
    Prevents:
	
		Buffer overflow attacks
		
		Code injection
		
		Privilege escalation
		
		Enforces process isolation and sandboxing.
		
		Reduces impact of compromised or buggy programs.

83. Explain memory isolation.
	
	Memory isolation ensures that one process cannot read/write to the memory of another process or the kernel. Achieved using:
	
		Virtual memory
		
		Separate page tables
		
		Hardware privilege levels

84. Challenges in implementing memory protection:
	
 	Overhead of managing page tables and TLBs.
	
	Supporting shared memory without violating isolation.
	
	Performance impact of frequent context switching.
	
	Complexity in debugging access violations.
	
	Granularity of protection (page size limits flexibility).

85. How does memory protection contribute to system security?

	Prevents malware or user programs from corrupting the system.
	
	Enforces least privilege principle.
	
	Detects and isolates faulty behavior (via segfaults).
	
	Enables secure multi-user and multi-process environments.

86. C Program: Dynamic Memory Allocation using malloc()

	#include <stdio.h>
	#include <stdlib.h>
	
	int main() {
	    int *ptr = (int *)malloc(sizeof(int));
	    if (ptr == NULL) {
	        printf("Memory allocation failed.\n");
	        return 1;
	    }
	
	    *ptr = 42;
	    printf("Value = %d\n", *ptr);
	
	    free(ptr); // Deallocate memory
	    return 0;
	}

87. C Program: Allocate Memory for Array using calloc()

	#include <stdio.h>
	#include <stdlib.h>
	
	int main() {
	    int n = 5;
	    int *arr = (int *)calloc(n, sizeof(int));
	    if (arr == NULL) {
	        printf("Memory allocation failed.\n");
	        return 1;
	    }
	
	    printf("Array values (default):\n");
	    for (int i = 0; i < n; i++) {
	        printf("%d ", arr[i]); // Should print 0s
	    }
	
	    free(arr);
	    return 0;
	}

88. C Program: Resize Dynamically Allocated Memory using realloc()

	#include <stdio.h>
	#include <stdlib.h>
	
	int main() {
	    int *arr = (int *)malloc(3 * sizeof(int));
	    if (arr == NULL) return 1;
	
	    for (int i = 0; i < 3; i++) arr[i] = i + 1;
	
	    // Resize array to hold 5 integers
	    arr = (int *)realloc(arr, 5 * sizeof(int));
	    if (arr == NULL) return 1;
	
	    arr[3] = 4;
	    arr[4] = 5;
	
	    printf("Resized array:\n");
	    for (int i = 0; i < 5; i++) {
	        printf("%d ", arr[i]);
	    }
	
	    free(arr);
	    return 0;
	}
 
89. C Program: Allocate Memory for a Linked List Node

	#include <stdio.h>
	#include <stdlib.h>
	
	struct Node {
	    int data;
	    struct Node *next;
	};
	
	int main() {
	    struct Node *head = (struct Node *)malloc(sizeof(struct Node));
	    if (head == NULL) return 1;
	
	    head->data = 100;
	    head->next = NULL;
	
	    printf("Node data: %d\n", head->data);
	
	    free(head);
	    return 0;
	}

90. First-Fit Memory Allocation Simulation in C

	#include <stdio.h>
	#define MAX 5
	
	int memory[MAX] = {0, 0, 0, 0, 0}; // 0 means free, 1 means allocated
	
	void first_fit(int size) {
	    int i, count = 0, start = -1;
	    for (i = 0; i < MAX; i++) {
	        if (memory[i] == 0) {
	            if (start == -1) start = i;
	            count++;
	            if (count == size) {
	                for (int j = start; j < start + size; j++)
	                    memory[j] = 1;
	                printf("Allocated %d units at block %d\n", size, start);
	                return;
	            }
	        } else {
	            count = 0;
	            start = -1;
	        }
	    }
	    printf("No sufficient memory to allocate %d units\n", size);
	}
	
	void print_memory() {
	    printf("Memory: ");
	    for (int i = 0; i < MAX; i++) {
	        printf("%d ", memory[i]);
	    }
	    printf("\n");
	}
	
	int main() {
	    print_memory();
	    first_fit(2);
	    print_memory();
	    first_fit(3);
	    print_memory();
	    first_fit(1);
	    print_memory();
	    return 0;
	}
 
91. Best-Fit Memory Allocation Simulation in C
	
	#include <stdio.h>
	#define MAX 10
	
	int memory[MAX] = {0,0,1,1,0,0,0,1,0,0};
	
	typedef struct {
	    int start;
	    int length;
	} Block;
	
	void best_fit(int size) {
	    Block blocks[MAX];
	    int block_count = 0;
	    int i = 0;
	    // Find free blocks
	    while (i < MAX) {
	        if (memory[i] == 0) {
	            int start = i;
	            int length = 0;
	            while (i < MAX && memory[i] == 0) {
	                length++;
	                i++;
	            }
	            blocks[block_count].start = start;
	            blocks[block_count].length = length;
	            block_count++;
	        } else {
	            i++;
	        }
	    }
	    // Find best fit
	    int best_index = -1;
	    int best_size = MAX + 1;
	    for (i = 0; i < block_count; i++) {
	        if (blocks[i].length >= size && blocks[i].length < best_size) {
	            best_size = blocks[i].length;
	            best_index = i;
	        }
	    }
	    if (best_index == -1) {
	        printf("No sufficient memory to allocate %d units\n", size);
	        return;
	    }
	    // Allocate
	    for (i = blocks[best_index].start; i < blocks[best_index].start + size; i++)
	        memory[i] = 1;
	    printf("Allocated %d units at block %d\n", size, blocks[best_index].start);
	}
	
	void print_memory() {
	    printf("Memory: ");
	    for (int i = 0; i < MAX; i++) printf("%d ", memory[i]);
	    printf("\n");
	}
	
	int main() {
	    print_memory();
	    best_fit(2);
	    print_memory();
	    best_fit(3);
	    print_memory();
	    best_fit(4);
	    print_memory();
	    return 0;
	}
 
92. Worst-Fit Memory Allocation Simulation in C

	#include <stdio.h>
	#define MAX 10
	
	int memory[MAX] = {0,0,1,1,0,0,0,1,0,0};
	
	typedef struct {
	    int start;
	    int length;
	} Block;
	
	void worst_fit(int size) {
	    Block blocks[MAX];
	    int block_count = 0;
	    int i = 0;
	    // Find free blocks
	    while (i < MAX) {
	        if (memory[i] == 0) {
	            int start = i;
	            int length = 0;
	            while (i < MAX && memory[i] == 0) {
	                length++;
	                i++;
	            }
	            blocks[block_count].start = start;
	            blocks[block_count].length = length;
	            block_count++;
	        } else {
	            i++;
	        }
	    }
	    // Find worst fit
	    int worst_index = -1;
	    int worst_size = -1;
	    for (i = 0; i < block_count; i++) {
	        if (blocks[i].length >= size && blocks[i].length > worst_size) {
	            worst_size = blocks[i].length;
	            worst_index = i;
	        }
	    }
	    if (worst_index == -1) {
	        printf("No sufficient memory to allocate %d units\n", size);
	        return;
	    }
	    // Allocate
	    for (i = blocks[worst_index].start; i < blocks[worst_index].start + size; i++)
	        memory[i] = 1;
	    printf("Allocated %d units at block %d\n", size, blocks[worst_index].start);
	}
	
	void print_memory() {
	    printf("Memory: ");
	    for (int i = 0; i < MAX; i++) printf("%d ", memory[i]);
	    printf("\n");
	}
	
	int main() {
	    print_memory();
	    worst_fit(2);
	    print_memory();
	    worst_fit(3);
	    print_memory();
	    worst_fit(4);
	    print_memory();
	    return 0;
	}
93. Next-Fit Memory Allocation Simulation in C

	#include <stdio.h>
	#define MAX 10
	
	int memory[MAX] = {0};
	int last_allocated_index = 0;
	
	void next_fit(int size) {
	    int i = last_allocated_index;
	    int count = 0, start = -1;
	    int checked = 0;
	    while (checked < MAX) {
	        int idx = i % MAX;
	        if (memory[idx] == 0) {
	            if (start == -1) start = idx;
	            count++;
	            if (count == size) {
	                for (int j = start; j < start + size; j++)
	                    memory[j % MAX] = 1;
	                printf("Allocated %d units at block %d\n", size, start);
	                last_allocated_index = (start + size) % MAX;
	                return;
	            }
	        } else {
	            count = 0;
	            start = -1;
	        }
	        i++;
	        checked++;
	    }
	    printf("No sufficient memory to allocate %d units\n", size);
	}
	
	void print_memory() {
	    printf("Memory: ");
	    for (int i = 0; i < MAX; i++) printf("%d ", memory[i]);
	    printf("\n");
	}
	
	int main() {
	    print_memory();
	    next_fit(3);
	    print_memory();
	    next_fit(2);
	    print_memory();
	    next_fit(4);
	    print_memory();
	    return 0;
	}
 
94. Simple Buddy System Allocator (Conceptual)
	A complete buddy system implementation is quite extensive. Here's a very simplified skeleton to illustrate the idea:

	#include <stdio.h>
	#include <stdlib.h>
	#include <math.h>
	
	#define MAX_ORDER 4  // max block size = 2^MAX_ORDER units
	
	typedef struct block {
	    int size;
	    int free;
	    struct block* next;
	} block_t;
	
	block_t* free_lists[MAX_ORDER+1];
	
	void init() {
	    for(int i=0; i<=MAX_ORDER; i++) free_lists[i] = NULL;
	    // Start with one big free block of max size
	    block_t* b = malloc(sizeof(block_t));
	    b->size = 1 << MAX_ORDER;
	    b->free = 1;
	    b->next = NULL;
	    free_lists[MAX_ORDER] = b;
	}
	
	int order_of_size(int size) {
	    int order = 0;
	    int s = 1;
	    while (s < size) {
	        s <<= 1;
	        order++;
	    }
	    return order;
	}
	
	void split(int order) {
	    if(order == 0 || free_lists[order] == NULL) return;
	    block_t* block = free_lists[order];
	    free_lists[order] = block->next;
	    int half_size = block->size / 2;
	
	    block_t* left = block;
	    left->size = half_size;
	    left->free = 1;
	    left->next = NULL;
	
	    block_t* right = malloc(sizeof(block_t));
	    right->size = half_size;
	    right->free = 1;
	    right->next = NULL;
	
	    free_lists[order-1] = left;
	    left->next = right;
	}
	
	block_t* buddy_alloc(int size) {
	    int order = order_of_size(size);
	    for (int i = order; i <= MAX_ORDER; i++) {
	        if (free_lists[i] != NULL) {
	            while (i > order) {
	                split(i);
	                i--;
	            }
	            block_t* block = free_lists[order];
	            free_lists[order] = block->next;
	            block->free = 0;
	            block->next = NULL;
	            printf("Allocated block size %d\n", block->size);
	            return block;
	        }
	    }
	    printf("No free block available\n");
	    return NULL;
	}
	
	int main() {
	    init();
	    block_t* b1 = buddy_alloc(5);
	    block_t* b2 = buddy_alloc(3);
	    return 0;
	}
 
95. Custom Memory Management Algorithm
	Here’s a simple example using a fixed-size block allocator with a free list:

	#include <stdio.h>
	#include <stdlib.h>
	
	#define MEMORY_SIZE 1024
	#define BLOCK_SIZE 32
	#define BLOCKS (MEMORY_SIZE / BLOCK_SIZE)
	
	char memory[MEMORY_SIZE];
	int free_list[BLOCKS];
	
	void init() {
	    for (int i=0; i<BLOCKS; i++) {
	        free_list[i] = 1; // 1 means free
	    }
	}
	
	void* custom_alloc() {
	    for (int i=0; i<BLOCKS; i++) {
	        if (free_list[i]) {
	            free_list[i] = 0;
	            printf("Allocating block %d\n", i);
	            return &memory[i*BLOCK_SIZE];
	        }
	    }
	    printf("Out of memory\n");
	    return NULL;
	}
	
	void custom_free(void* ptr) {
	    int block = ((char*)ptr - memory) / BLOCK_SIZE;
	    free_list[block] = 1;
	    printf("Freed block %d\n", block);
	}
	
	int main() {
	    init();
	    void* p1 = custom_alloc();
	    void* p2 = custom_alloc();
	    custom_free(p1);
	    void* p3 = custom_alloc();
	    return 0;
	}
 
96. Demonstrate mmap() in C

	#include <stdio.h>
	#include <sys/mman.h>
	#include <fcntl.h>
	#include <unistd.h>
	
	int main() {
	    int fd = open("testfile", O_RDWR | O_CREAT, 0666);
	    ftruncate(fd, 4096); // Set file size
	
	    char* addr = mmap(NULL, 4096, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0);
	    if (addr == MAP_FAILED) {
	        perror("mmap");
	        return 1;
	    }
	
	    addr[0] = 'H';
	    addr[1] = 'i';
	    addr[2] = '\0';
	
	    printf("Mapped memory content: %s\n", addr);
	
	    munmap(addr, 4096);
	    close(fd);
	    return 0;
	}
 
97. Read and Write to a Memory-Mapped File
	
	#include <stdio.h>
	#include <fcntl.h>
	#include <sys/mman.h>
	#include <unistd.h>
	#include <string.h>
	
	int main() {
	    int fd = open("testfile", O_RDWR | O_CREAT, 0666);
	    ftruncate(fd, 4096);
	
	    char* data = mmap(NULL, 4096, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0);
	    if (data == MAP_FAILED) {
	        perror("mmap");
	        return 1;
	    }
	
	    // Write to mmap
	    strcpy(data, "Hello mmap!");
	
	    // Read from mmap
	    printf("Content: %s\n", data);
	
	    munmap(data, 4096);
	    close(fd);
	    return 0;
	}
 
98. Shared Memory with shmget() and shmat()

	#include <stdio.h>
	#include <sys/ipc.h>
	#include <sys/shm.h>
	#include <string.h>
	
	#define SHM_SIZE 1024
	
	int main() {
	    key_t key = ftok("shmfile",65);
	    int shmid = shmget(key, SHM_SIZE, 0666|IPC_CREAT);
	    char *str = (char*) shmat(shmid, NULL, 0);
	
	    strcpy(str, "Hello Shared Memory!");
	    printf("Data written to shared memory: %s\n", str);
	
	    shmdt(str);
	    return 0;
	}
 
99. Shared Memory with Semaphore Synchronization
	This requires two processes; here’s a simple example with semaphore:

	#include <stdio.h>
	#include <sys/ipc.h>
	#include <sys/shm.h>
	#include <sys/sem.h>
	#include <string.h>
	#include <unistd.h>
	
	#define SHM_SIZE 1024
	
	union semun {
	    int val;
	    struct semid_ds *buf;
	    unsigned short *array;
	};
	
	int main() {
	    key_t key = ftok("shmfile",65);
	    int shmid = shmget(key, SHM_SIZE, 0666|IPC_CREAT);
	    char *str = (char*) shmat(shmid, NULL, 0);
	
	    int semid = semget(key, 1, 0666 | IPC_CREAT);
	
	    union semun u;
	    u.val = 1;
	    semctl(semid, 0, SETVAL, u);
	
	    struct sembuf p = {0, -1, SEM_UNDO}; // wait
	    struct sembuf v = {0, 1, SEM_UNDO};  // signal
	
	    semop(semid, &p, 1); // wait
	    strcpy(str, "Data with semaphore sync!");
	    printf("Written: %s\n", str);
	    semop(semid, &v, 1); // signal
	
	    shmdt(str);
	    return 0;
	}
 
100. Page Replacement Algorithms Simulator (FIFO, LRU, Optimal)
	Here is a simple FIFO implementation. Others can be extended similarly.

	#include <stdio.h>
	#define MAX_FRAMES 3
	#define MAX_PAGES 12
	
	void fifo(int pages[], int n) {
	    int frames[MAX_FRAMES];
	    int i, j, k, pos = 0, faults = 0;
	    for (i = 0; i < MAX_FRAMES; i++) frames[i] = -1;
	
	    for (i = 0; i < n; i++) {
	        int found = 0;
	        for (j = 0; j < MAX_FRAMES; j++) {
	            if (frames[j] == pages[i]) {
	                found = 1;
	                break;
	            }
	        }
	        if (!found) {
	            frames[pos] = pages[i];
	            pos = (pos + 1) % MAX_FRAMES;
	            faults++;
	            printf("Page %d caused a fault. Frames: ", pages[i]);
	            for (k = 0; k < MAX_FRAMES; k++)
	                printf("%d ", frames[k]);
	            printf("\n");
	        }
	    }
	    printf("Total page faults: %d\n", faults);
	}
	
	int main() {
	    int pages[MAX_PAGES] = {7, 0, 1, 2, 0, 3, 0, 4, 2, 3, 0, 3};
	    int n = MAX_PAGES;
	    fifo(pages, n);
	    return 0;
	}
